{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilentClaw27/AdvancedRL/blob/main/lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NcKK5kqMJogc"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Sequence\n",
        "from collections import namedtuple, deque\n",
        "import itertools\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nI3vEhXkk8GL"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 10000\n",
        "MIN_REPLAY_SIZE = 5000\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 0.995\n",
        "TARGET_UPDATE_FREQ = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hyY-bUr7jtdC"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "obs, info = env.reset()\n",
        "episode_reward = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gwr6u6u6j4DL"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'dones', 'next_states'))\n",
        "\n",
        "class Replay_memory():\n",
        "  def __init__(self, env, fullsize, minsize, batchsize):\n",
        "    self.env = env\n",
        "    self.memory = deque(maxlen=fullsize)\n",
        "    self.rewards = deque(maxlen=50)\n",
        "    self.batchsize = batchsize\n",
        "    self.minsize = minsize\n",
        "  \n",
        "  def append(self,transition):\n",
        "    self.memory.append(transition)\n",
        "\n",
        "  def sample_batch(self):\n",
        "      batch = random.sample(self.memory, self.batchsize)\n",
        "      batch = Transition(*zip(*batch))\n",
        "      states = torch.from_numpy(np.array(batch.states, dtype=np.float32))\n",
        "      actions = torch.from_numpy(np.array(batch.actions, dtype=np.int64)).unsqueeze(1)\n",
        "      rewards = torch.from_numpy(np.array(batch.rewards, dtype=np.float32)).unsqueeze(1)\n",
        "      dones = torch.from_numpy(np.array(batch.dones, dtype=np.bool8)).unsqueeze(1)\n",
        "      next_states = torch.from_numpy(np.array(batch.next_states, dtype=np.float32))\n",
        "      return states, actions, rewards, dones, next_states\n",
        "\n",
        "  def initialize(self):\n",
        "    obs, info = env.reset()\n",
        "    for _ in range(self.minsize):\n",
        "        action = self.env.action_space.sample()\n",
        "        new_obs, reward, done, info = env.step(action)[:4]\n",
        "        transition = Transition(obs, action, reward, done, new_obs)\n",
        "        self.append(transition)\n",
        "        obs = new_obs\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "    return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_gUTGnHtj4U_"
      },
      "outputs": [],
      "source": [
        "replay_memory = Replay_memory(env, BUFFER_SIZE, MIN_REPLAY_SIZE, BATCH_SIZE).initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YDYNYHs1j4ZF"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, ninputs, noutputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.a1 = nn.Linear(ninputs, 64)\n",
        "        self.a2 = nn.Linear(64, noutputs)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        o = self.a1(X)\n",
        "        o = F.relu(o)\n",
        "        o = self.a2(o)\n",
        "        return o\n",
        "    \n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOK4T49bmPqN",
        "outputId": "7323024d-ecbf-4a19-d0db-30eb12b5fd98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (a1): Linear(in_features=8, out_features=64, bias=True)\n",
              "  (a2): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dqn_policy = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "dqn_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "dqn_target.load_state_dict(dqn_policy.state_dict())\n",
        "dqn_target.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rueEzZOomPzX"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(dqn_policy.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gqTd0j_imVa0"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(epsilon, obs):\n",
        "    rnd_sample = random.random()\n",
        "    if rnd_sample <= epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            action = int(torch.argmax(dqn_policy(torch.Tensor(obs))))\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lUdcvwspLPA",
        "outputId": "305ee25e-21dc-43fd-b929-ff30e66c3729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 50 Avg Results: -122.37846243139677 Epsilon: 0.7822236754458713\n",
            "Episode: 100 Avg Results: -73.41797039904563 Epsilon: 0.6088145090359074\n",
            "Episode: 150 Avg Results: -55.79774112326688 Epsilon: 0.4738479773082268\n",
            "Episode: 200 Avg Results: -52.972174638869035 Epsilon: 0.36880183088056995\n",
            "Episode: 250 Avg Results: -24.799709340111573 Epsilon: 0.28704309604425327\n",
            "Episode: 300 Avg Results: 46.57133272405362 Epsilon: 0.22340924607110255\n",
            "Episode: 350 Avg Results: 155.9346742371871 Epsilon: 0.17388222158237718\n",
            "Solved at episode: 372 Avg Results: 195.77401234069188\n"
          ]
        }
      ],
      "source": [
        "obs, info = env.reset()\n",
        "eps_threshold =EPS_START\n",
        "episode = 1\n",
        "\n",
        "for step in itertools.count():\n",
        "    action = epsilon_greedy_policy(eps_threshold, obs)\n",
        "    new_obs, reward, terminated,truncated, _ = env.step(action)\n",
        "    replay_memory.append(Transition(obs, action, reward, terminated, new_obs))\n",
        "    episode_reward += reward\n",
        "    obs = new_obs\n",
        "    \n",
        "    if terminated or truncated:\n",
        "        episode += 1\n",
        "        eps_threshold = np.max((eps_threshold*EPS_DECAY, EPS_END))\n",
        "        replay_memory.rewards.append(episode_reward)\n",
        "        obs,info = env.reset()\n",
        "        avg_res = np.mean(replay_memory.rewards)\n",
        "\n",
        "        if episode % 50 == 0: \n",
        "            avg_res = np.mean(replay_memory.rewards)\n",
        "            print(f'Episode: {episode} Avg Results: {avg_res} Epsilon: {eps_threshold}')\n",
        "\n",
        "        if avg_res >= 195 :\n",
        "            print(f'Solved at episode: {episode} Avg Results: {avg_res}')\n",
        "            break\n",
        "        \n",
        "        if step % TARGET_UPDATE_FREQ == 0:\n",
        "            dqn_target.load_state_dict(dqn_policy.state_dict())\n",
        "\n",
        "        episode_reward = 0\n",
        "\n",
        "    b_states, b_actions, b_rewards, b_dones, b_next_states = replay_memory.sample_batch()\n",
        "\n",
        "    qvalues = dqn_policy(b_states).gather(1, b_actions)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        target_qvalues = dqn_target(b_next_states)\n",
        "        max_target_qvalues = torch.max(target_qvalues, axis=1).values.unsqueeze(1)\n",
        "        expected_qvalues = b_rewards + GAMMA * (1 - b_dones.type(torch.int64)) * max_target_qvalues\n",
        "\n",
        "    loss = loss_fn(qvalues, expected_qvalues)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in dqn_policy.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KqVYImJnd_H"
      },
      "outputs": [],
      "source": [
        "# for episode in range(10):\n",
        "#   done = False\n",
        "#   state = env.reset()\n",
        "\n",
        "#   while not done:\n",
        "#     action = env.action_space.sample()\n",
        "#     state, _, done, _ = env.step(action)\n",
        "\n",
        "# del env"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e60e170a43f951995987d5f861e903e8cddcd13fd622fefc94b6b25d6f35233f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

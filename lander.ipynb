{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilentClaw27/AdvancedRL/blob/main/lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "  gym==0.21\\\n",
        "  gym[box2d]==0.21\\\n",
        "  pyvirtualdisplay\\\n",
        "  pyglet==1.5.27 \\\n",
        "  colabgymrender"
      ],
      "metadata": {
        "id": "wG_5lJH9sCb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pyglet\n",
        "from gym.wrappers import RecordVideo\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Sequence\n",
        "from collections import namedtuple, deque\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "NcKK5kqMJogc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "MIN_REPLAY_SIZE = 5000\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 0.995\n",
        "TARGET_UPDATE_FREQ = 5"
      ],
      "metadata": {
        "id": "nI3vEhXkk8GL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env = RecordVideo(env, 'videos', episode_trigger=lambda e: True)\n",
        "\n",
        "state = env.reset()\n",
        "e_reward = 0.0"
      ],
      "metadata": {
        "id": "hyY-bUr7jtdC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'dones', 'next_states'))\n",
        "\n",
        "class Replay_memory():\n",
        "  def __init__(self, env, fullsize, minsize, batchsize):\n",
        "    self.env = env\n",
        "    self.memory = deque(maxlen=fullsize)\n",
        "    self.rewards = deque(maxlen=50)\n",
        "    self.batchsize = batchsize\n",
        "    self.minsize = minsize\n",
        "  \n",
        "  def append(self,transition):\n",
        "    self.memory.append(transition)\n",
        "\n",
        "  def sample_batch(self):\n",
        "      batch = random.sample(self.memory, self.batchsize)\n",
        "      batch = Transition(*zip(*batch))\n",
        "      states = torch.from_numpy(np.array(batch.states, dtype=np.float32))\n",
        "      actions = torch.from_numpy(np.array(batch.actions, dtype=np.int64)).unsqueeze(1)\n",
        "      rewards = torch.from_numpy(np.array(batch.rewards, dtype=np.float32)).unsqueeze(1)\n",
        "      dones = torch.from_numpy(np.array(batch.dones, dtype=np.bool8)).unsqueeze(1)\n",
        "      next_states = torch.from_numpy(np.array(batch.next_states, dtype=np.float32))\n",
        "      return states, actions, rewards, dones, next_states\n",
        "\n",
        "  def initialize(self):\n",
        "    obs = env.reset()\n",
        "    for _ in range(self.minsize):\n",
        "        action = self.env.action_space.sample()\n",
        "        new_obs, reward, done, info = env.step(action)\n",
        "        transition = Transition(obs, action, reward, done, new_obs)\n",
        "        self.append(transition)\n",
        "        obs = new_obs\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "    return self"
      ],
      "metadata": {
        "id": "Gwr6u6u6j4DL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_memory = Replay_memory(env, BUFFER_SIZE, MIN_REPLAY_SIZE, BATCH_SIZE).initialize()"
      ],
      "metadata": {
        "id": "_gUTGnHtj4U_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, ninputs, noutputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.a1 = nn.Linear(ninputs, 64)\n",
        "        self.a2 = nn.Linear(64, noutputs)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        o = self.a1(X)\n",
        "        o = torch.tanh(o)\n",
        "        o = self.a2(o)\n",
        "        return o\n",
        "    \n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)"
      ],
      "metadata": {
        "id": "YDYNYHs1j4ZF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_policy = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "dqn_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "dqn_target.load_state_dict(dqn_policy.state_dict())\n",
        "dqn_target.eval()"
      ],
      "metadata": {
        "id": "aOK4T49bmPqN",
        "outputId": "7323024d-ecbf-4a19-d0db-30eb12b5fd98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (a1): Linear(in_features=8, out_features=64, bias=True)\n",
              "  (a2): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.SmoothL1Loss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(dqn_policy.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "rueEzZOomPzX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(epsilon, obs):\n",
        "    rnd_sample = random.random()\n",
        "    if rnd_sample <= epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            action = int(torch.argmax(dqn_policy(torch.Tensor(obs))))\n",
        "    return action"
      ],
      "metadata": {
        "id": "gqTd0j_imVa0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "eps_threshold =EPS_START\n",
        "episode = 1\n",
        "\n",
        "for step in itertools.count():\n",
        "    action = epsilon_greedy_policy(eps_threshold, obs)\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "    replay_memory.append(Transition(obs, action, reward, done, new_obs))\n",
        "    e_reward += reward\n",
        "    obs = new_obs\n",
        "    \n",
        "    if done:\n",
        "        episode += 1\n",
        "        eps_threshold = np.max((eps_threshold*EPS_DECAY, EPS_END))\n",
        "        replay_memory.rewards.append(e_reward)\n",
        "        obs = env.reset()\n",
        "        avg_res = np.mean(replay_memory.rewards)\n",
        "\n",
        "        if episode % 50 == 0: \n",
        "            avg_res = np.mean(replay_memory.rewards)\n",
        "            print(f'Episode: {episode} Avg Results: {avg_res} Epsilon: {eps_threshold}')\n",
        "\n",
        "        if avg_res >= 195 :\n",
        "            print(f'Solved at episode: {episode} Avg Results: {avg_res}')\n",
        "            break\n",
        "        \n",
        "        if step % TARGET_UPDATE_FREQ == 0:\n",
        "            dqn_target.load_state_dict(dqn_policy.state_dict())\n",
        "\n",
        "        episode_reward = 0\n",
        "\n",
        "    b_states, b_actions, b_rewards, b_dones, b_next_states = replay_memory.sample_batch()\n",
        "\n",
        "    qvalues = dqn_policy(b_states).gather(1, b_actions)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        target_qvalues = dqn_target(b_next_states)\n",
        "        max_target_qvalues = torch.max(target_qvalues, axis=1).values.unsqueeze(1)\n",
        "        expected_qvalues = b_rewards + GAMMA * (1 - b_dones.type(torch.int64)) * max_target_qvalues\n",
        "\n",
        "    loss = loss_fn(qvalues, expected_qvalues)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in dqn_policy.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "0lUdcvwspLPA",
        "outputId": "305ee25e-21dc-43fd-b929-ff30e66c3729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 50 Avg Results: -3766.069586724057 Epsilon: 0.7822236754458713\n",
            "Episode: 100 Avg Results: -8908.689987152695 Epsilon: 0.6088145090359074\n",
            "Episode: 150 Avg Results: -11614.187731973845 Epsilon: 0.4738479773082268\n",
            "Episode: 200 Avg Results: -12041.995406489908 Epsilon: 0.36880183088056995\n",
            "Episode: 250 Avg Results: -12240.544830860514 Epsilon: 0.28704309604425327\n",
            "Episode: 300 Avg Results: -11380.08800536544 Epsilon: 0.22340924607110255\n",
            "Episode: 350 Avg Results: -9378.474030956062 Epsilon: 0.17388222158237718\n",
            "Episode: 400 Avg Results: -4783.236330901511 Epsilon: 0.1353347165085562\n",
            "Solved at episode: 436 Avg Results: 279.29580320292547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KqVYImJnd_H"
      },
      "outputs": [],
      "source": [
        "# for episode in range(10):\n",
        "#   done = False\n",
        "#   state = env.reset()\n",
        "\n",
        "#   while not done:\n",
        "#     action = env.action_space.sample()\n",
        "#     state, _, done, _ = env.step(action)\n",
        "\n",
        "# del env"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5bab6980fee78fae71063a5c532eb71480fd5c5c2d857272c587d3182c09e720"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}